#è¦æƒ³help(è‡ªåˆ›å‡½æ•°)æœ‰æ˜¾ç¤ºï¼Œåˆ™è¦åœ¨è‡ªåˆ›å‡½æ•°é‡Œé¢æ³¨é‡Šã€‚

#help(planets.append) appendå•ç‹¬å¯¹Listèµ·ä½œç”¨   æ‰€ä»¥ç”¨helpæ—¶è¦åœ¨appendå‰åŠ å¯¹åº”list.

#åˆ¤æ–­æ˜¯å¦æŸå…ƒç´ åœ¨åˆ—è¡¨å†… ys in list

#åœ¨è‡ªåˆ›å‡½æ•°æœ€åreturn a < 0 å¯åˆ¤æ–­æ˜¯å¦è¾“å…¥æ•°ä¸ºè´Ÿæ•°

#return int(cheese + pizza + hotdog) å¯åˆ¤æ–­é€‰å“ªä¸€ä¸ª

#list
x.bit_length()è¿”å›äºŒè¿›åˆ¶éœ€è¦å¤šå°‘ä½æ•°
x.imag è¿”å›è™šéƒ¨

#tuple
	x = 0.125
	x.as_integer_ratio()
	(1, 8)
	
	numerator, denominator = x.as_integer_ratio()
	print(numerator / denominator)
	0.125


#if char.isupper()å¯ä»¥æ‰¾å‡ºå¤§å†™å•è¯

#loud_short_planets = [planet.upper() + '!' for planet in planets if len(planet) < 6]
== [
    planet.upper() + '!' 
    for planet in planets 
    if len(planet) < 6
]

#[32 for planet in planets]
#return any([num % 7 == 0 for num in nums])
return any[i for i in range(len(meals)-1) if meals[i]==meals[i+1]]  è¿™ç§æƒ…å†µiæ˜¯æ•°å­—ï¼Œä¸èƒ½è¡¨è¾¾boolï¼Œæ‰€ä»¥è¯­æ³•é”™è¯¯
return any(meals[i] == meals[i+1] for i in range(len(meals)-1)) ä¸€èˆ¬ç›´æ¥ä¸Šæ¡ä»¶

*ä¸€ä¸ªforå¾ªç¯é‡Œå‡ºç°returnå³ç»“æŸå¾ªç¯ï¼Œæ‰€ä»¥è¦æ³¨æ„å‡ºç°ä¸¤ä¸ªreturnçš„æƒ…å†µï¼Œä¸€èˆ¬ä¾¿åˆ©åªæœ‰ä¸€ä¸ªreturnåœ¨foré‡Œé¢

========================================STRING AND DIC============================================
STRING
# ALL CAPS
claim = "Pluto is a planet!"
claim.upper()

# all lowercase
claim.lower()

# Searching for the first index of a substring
in:claim.index('plan')
out:11

#in:claim.startswith(planet)
 out:True

#words = claim.split()
words
['Pluto', 'is', 'a', 'planet!']

# Yes, we can put unicode characters right in our string literals :)
' ğŸ‘ '.join([word.upper() for word in words])

datestr = '1956-01-31'
year, month, day = datestr.split('-')
'/'.join([month, day, year])
out:'01/31/1956'

#è¿æ¥å­—ç¬¦ä¸² planet + ", you'll always be the " + str(position) + "th planet to me."(position=9)

s = """Pluto's a {0}.
No, it's a {1}.
{0}!
{1}!""".format('planet', 'dwarf planet')
print(s)

out:Pluto's a planet.
No, it's a dwarf planet.
planet!
dwarf planet!

#def is_valid_zip(zip_code):
    return len(zip_code) == 5 and zip_code.isdigit() ååŠéƒ¨åˆ†æ£€æµ‹æ˜¯å¦ä¸ºæ•´æ•°
ä»å¤–åˆ°å†…ï¼Œèƒ½å¦ç›´æ¥ä»å†…åˆ°å¤–


***for i, doc in enumerate(doc_list):
        # Split the string doc into a list of words (according to whitespace)
        tokens = doc.split()
        # Make a transformed list where we 'normalize' each word to facilitate matching.
        # Periods and commas are removed from the end of each word, and it's set to all lowercase.
        normalized = [token.rstrip('.,').lower() for token in tokens

========================================panda==================================================
*
pd.set_option("display.max_rows", 5)


pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 
              'Sue': ['Pretty good.', 'Bland.']},
             index=['Product A', 'Product B'])

pd.Series([1, 2, 3, 4, 5])

pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')


review = pd.read_csv()

review.to_csv('./')

#æ–¹ä¾¿è®°å½•review.shape().head()

reviews.set_index("title")

#åˆ¤æ–­æ£€æµ‹
reviews.country == 'Italy'

reviews.loc[reviews.country == 'Italy']

reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)] and

reviews.loc[(reviews.country == 'Italy') | (reviews.points >= 90)] or

reviews.loc[reviews.price.notnull()] isnull()

reviews.loc[reviews.country.isin(['Italy', 'France'])]

# æ’åº&æ–°å¢åˆ—
reviews['index_backwards'] = range(len(reviews), 0, -1) //å°†reviewsä¸­çš„ç´¢å¼•å€¼ï¼ˆä»0å¼€å§‹ï¼‰æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œç»“æœå­˜å‚¨åœ¨reviews['index_backwards']

# å¿«æ·å‘½ä»¤
describe
mean 
value_counts() #ç®—ä¸€ä¸‹æ¯ä¸ªç§ç±»å‡ºç°çš„æ¬¡æ•°
unique#å¯ä»¥ä¸é‡å¤çš„è¾“å‡ºä¸€æ¡æœ‰ä»€ä¹ˆç±»å‹ 
median

Map
é€šè¿‡å‡½æ•°æ˜ å°„åˆ—ä¸­çš„å€¼,ä¾‹å¦‚,å°†åˆ—â€œpointsâ€ä¸­çš„æ¯ä¸ªå€¼å‡å»å…¶å¹³å‡å€¼
review_points_mean = reviews.points.mean()
1.
reviews.points.map(lambda p: p - review_points_mean) == review.points - review_points_mean
2.
def remean_points(row):
    row.points = row.points - review_points_mean
    return row

reviews.apply(remean_points, axis='columns') applyå®ƒä¼šè‡ªé€‚åº”å‡½æ•°é‡Œé¢çš„åˆ—ğŸ˜…ï¼Œç„¶åæ²¡æœ‰returnåªèƒ½è¿”å›Noneï¼Œå®ƒä¸ä¼šä¿®æ”¹dataframeï¼Œ
3.
reviews.description.map(lambda desc: "tropical" in desc).sum()

\è¿ç®—
review_points_mean = reviews.points.mean()
reviews.points - review_points_mean

reviews.country + " - " + reviews.region_1  // æ‹¼æ¥åˆ—

'''
Create variable centered_price containing a version of the price column with the mean price subtracted.

(Note: this 'centering' transformation is a common preprocessing step before applying various machine learning algorithms.)'''

reviews.groupby('points').points.count()

reviews.groupby('points').price.min()

reviews.groupby('winery').apply(lambda df: df.title.iloc[0]) //ç¬¬ä¸€ä¸ªçš„title

reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()]) //æœ€å¤§å€¼

reviews.groupby(['country']).price.agg([len, min, max]) //more effective

countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len])
countries_reviewed

countries_reviewed.reset_index()

countries_reviewed = countries_reviewed.reset_index()
countries_reviewed.sort_values(by='len')

countries_reviewed.sort_values(by='len', ascending=False)

countries_reviewed.sort_index()  // æŒ‰ç´¢å¼•æ’åº

countries_reviewed.sort_values(by=['country', 'len'])  //a-z

size() #ç›´æ¥å°†dataframeå˜æˆseries

reviews.points.astype('float64')

reviews.dtypes  reviews.index.dtype  reviews[pd.isnull(reviews.country)]  reviews.region_2.fillna("Unknown")#è¿™é‡Œæ˜¯æŠŠæ²¡æœ‰æ•°å€¼çš„éƒ½å¡«ä¸Š

reviews.taster_twitter_handle.replace("@kerinokeefe", "@kerino")

reviews.rename(columns={'points': 'score'})

reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})

reviews.rename_axis("wines", axis='rows').rename_axis("fields", axis='columns')

pd.concat([canadian_youtube, british_youtube])

left = canadian_youtube.set_index(['title', 'trending_date'])
right = british_youtube.set_index(['title', 'trending_date'])

left.join(right, lsuffix='_CAN', rsuffix='_UK')

*ymistake
bargain_idx = (reviews.points / reviews.price).idxmax()
bargain_wine = reviews.loc[bargain_idx, 'title']



=========================æ•°æ®å¯è§†åŒ–============================
#åˆå§‹åŒ–
import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
print("Setup Complete")

spotify_data = pd.read_csv(spotify_filepath, index_col="Date", parse_dates=True)
#æ£€æŸ¥æ•°æ®ç¤ºä¾‹
spotify_data.head()
spotify_data.tail()
list(spotify_data.columns)

#æ–¹æ³•
sns.lineplot(data=spotify_data['Shape of You'], label="Shape of You") //é€‚ç”¨äºæç»˜æ—¶é—´åˆ—çš„æ•°å­—å˜åŒ–
plt.figure(figsize=(14,6))
plt.title("")
plt.xlabel("")

sns.barplot(x=flight_data.index, y=flight_data['NK']), #ä¸€èˆ¬xä¸ºdataçš„ç¬¬ä¸€åˆ—ï¼Œå³data.index,æ¡å½¢å›¾

sns.heatplot(data=)

sns.scatter(x=data[], y=data[], hue=data[]) #æ•£ç‚¹å›¾ï¼Œ hueçš„åŠŸèƒ½:é™¤äº†x,yä¸¤ä¸ªç‰¹è¯Šå¤–,é¢œè‰²æ ‡è®°æ˜¯å¦æœ‰ç¬¬ä¸‰ç‰¹å¾

sns.regplot(åŒä¸Š) #å›å½’,å¦‚æœä¸Šé¢çš„æ•£ç‚¹å›¾çœ‹ä¸å‡ºçº¿æ€§å…³ç³»,ç”¨è¿™ä¸ª

sns.lmplot(x="bmi", y="charges", hue="smoker", data=insurance_data) #åŒå›å½’,æœ‰æŸç‰¹å¾å’Œæ— è¯¥ç‰¹å¾

sns.swarmplot(x=insurance_data['smoker'],
              y=insurance_data['charges']) #ç”¨ç‰¹å®šè½´æœ‰æ— è¯¥ç‰¹å¾çš„æ¯”è¾ƒ
   
# Histograms for each species
sns.histplot(data=iris_data, x='Petal Length (cm)', hue='Species') #è¿™é‡Œç”¨äº†ä¸¤ä¸ªç‰¹è¯Š
# KDE plots for each species å¯†åº¦å›¾
sns.kdeplot(data=iris_data, x='Petal Length (cm)', hue='Species', shade=True)

# Add title
plt.title("Distribution of Petal Lengths, by Species")
# 2D KDE plot åŒæ—¶æ˜¾ç¤ºä¸¤ä¸ªç‰¹å¾çš„å¯†åº¦å›¾
sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind="kde")
# KDE plot 
sns.kdeplot(data=iris_data['Petal Length (cm)'], shade=True)
# Histogram 
sns.histplot(iris_data['Petal Length (cm)'])

# Change the style of the figure to the "dark" theme
sns.set_style("dark")
(1)"darkgrid", (2)"whitegrid", (3)"dark", (4)"white", and (5)"ticks"

===========================================data cleaning=========================================================              
# plotting modules
import seaborn as sns
import matplotlib.pyplot as plt

# set seed for reproducibility
np.random.seed(0)


// Handing Missing Value
# get the number of missing data points per column
missing_values_count = nfl_data.isnull().sum()

# look at the # of missing points in the first ten columns
missing_values_count[0:10]

# how many total missing values do we have?
total_cells = np.product(nfl_data.shape)
total_missing = missing_values_count.sum()

# percent of data that is missing
percent_missing = (total_missing/total_cells) * 100
print(percent_missing)


# replace all NA's the value that comes directly after it in the same column, 
# then replace all the remaining na's with 0
subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)

# replace all NA's with 0
subset_nfl_data.fillna(0)

# remove all columns with at least one missing value
columns_with_na_dropped = nfl_data.dropna(axis=1)
columns_with_na_dropped.head()

// Scaling and Normalization
# set up
# modules we'll use
import pandas as pd
import numpy as np

# for Box-Cox Transformation
from scipy import stats

# for min_max scaling
from mlxtend.preprocessing import minmax_scaling

eg1:Scaling
# generate 1000 data points randomly drawn from an exponential distribution
original_data = np.random.exponential(size=1000)

# mix-max scale the data between 0 and 1
scaled_data = minmax_scaling(original_data, columns=[0])

# plot both together to compare
fig, ax = plt.subplots(1, 2, figsize=(15, 3))
sns.histplot(original_data, ax=ax[0], kde=True, legend=False)
ax[0].set_title("Original Data")
sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)
ax[1].set_title("Scaled data")
plt.show()

eg2:Normalization
# normalize the exponential data with boxcox
normalized_data = stats.boxcox(original_data)
.......ä¸‹é¢ä»£ç ä¸ä¸Šé¢eg1è¿‘ä¼¼........

ex:Normalizationè¦æ±‚æ•°æ®å€¼å¤§äº1
positive = kickstarters_2017.pledged > 0
dataframe = kickstarters_2017.pledged.loc[positive]
series = pd.Series(stats.boxcox(dataframe)[0], 
                   name = 'pledged', index = dataframe.index)
                
ax = sns.histplot(series,kde = True)
ax.set_title('Normalize pledged')
plt.show()

// Parsing Dates
http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes
# create a new column, date_parsed, with the parsed dates
landslides['date_parsed'] = pd.to_datetime(landslides['date'], format="%m/%d/%y")

# auto but not effective
landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)
earthquakes['date_parsed'] = pd.to_datetime(earthquakes['Date'], format='mixed')
# get the day of the month from the date_parsed column èƒ½å¤Ÿè®©æˆ‘ä»¬çœ‹åˆ°æ¯ä¸ªæœˆä¸­çš„1-30å¤©çš„å‡ºç°é¢‘ç‡,è¿™é‡Œæ˜¯æ»‘å¡æ—¥æœŸ
day_of_month_landslides = landslides['date_parsed'].dt.day
day_of_month_landslides.head()

# remove na's
day_of_month_landslides = day_of_month_landslides.dropna()

# plot the day of the month
sns.distplot(day_of_month_landslides, kde=False, bins=31)

ex:
\\ This does appear to be an issue with data entry: ideally, all entries in the column have the same format.
We can get an idea of how widespread this issue is by checking the length of each entry in the "Date" column.

date_lengths = earthquakes.Date.str.len()
date_lengths.value_counts()

# find the unexpect index
indices = np.where([date_lengths == 24])[1]
print('Indices with corrupted data:', indices)
earthquakes.loc[indices]

----------------------------------Feature Engineering----------------------------

goal:æé«˜æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½;å‡å°‘è®¡ç®—æˆ–æ•°æ®éœ€æ±‚;æé«˜ç»“æœçš„å¯è§£é‡Šæ€§

# åœ¨ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹å¼€å§‹æ—¶ï¼Œå»ºç«‹è¿™æ ·çš„åŸºçº¿æ˜¯å¾ˆå¥½çš„åšæ³•ã€‚
# åŸºçº¿åˆ†æ•°å¯ä»¥å¸®åŠ©æ‚¨ç¡®å®šæ–°åŠŸèƒ½æ˜¯å¦å€¼å¾—ä¿ç•™ï¼Œæˆ–è€…æ˜¯å¦åº”è¯¥ä¸¢å¼ƒå®ƒä»¬å¹¶å¯èƒ½å°è¯•å…¶ä»–åŠŸèƒ½ã€‚

X = df.copy()
y = X.pop("CompressiveStrength")

# Train and score baseline model
baseline = RandomForestRegressor(criterion="absolute_error", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error"
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Baseline Score: {baseline_score:.4}")

# å¦‚æœä½ æ›¾ç»åœ¨å®¶åšé¥­ï¼Œä½ å¯èƒ½çŸ¥é“é£Ÿè°±ä¸­çš„æˆåˆ†æ¯”ä¾‹é€šå¸¸æ¯”å®ƒä»¬çš„ç»å¯¹æ•°é‡æ›´èƒ½é¢„æµ‹é£Ÿè°±çš„ç»“æœã€‚
# é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­ï¼Œä¸Šè¿°ç‰¹å¾çš„æ¯”ç‡å°†æ˜¯ CompressiveStrength çš„ä¸€ä¸ªå¾ˆå¥½çš„é¢„æµ‹æŒ‡æ ‡ã€‚

# ä¸‹é¢çš„å•å…ƒæ ¼å‘æ•°æ®é›†æ·»åŠ äº†ä¸‰ä¸ªæ–°çš„æ¯”ç‡è¦ç´ ã€‚

X = df.copy()
y = X.pop("CompressiveStrength")

# Create synthetic features
X["FCRatio"] = X["FineAggregate"] / X["CoarseAggregate"]
X["AggCmtRatio"] = (X["CoarseAggregate"] + X["FineAggregate"]) / X["Cement"]
X["WtrCmtRatio"] = X["Water"] / X["Cement"]

# Train and score model on dataset with additional ratio features
model = RandomForestRegressor(criterion="absolute_error", random_state=0)
score = cross_val_score(
    model, X, y, cv=5, scoring="neg_mean_absolute_error"
)
score = -1 * score.mean()

print(f"MAE Score with Ratio Features: {score:.4}")

# Mutual Information
é¢å¯¹æ–°æ•°æ®ï¼šä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨ç‰¹å¾æ•ˆç”¨æŒ‡æ ‡æ„å»ºæ’åï¼Œè¯¥æŒ‡æ ‡æ˜¯è¡¡é‡ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´å…³è”çš„å‡½æ•°ã€‚
ç„¶åï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä¸€å°éƒ¨åˆ†æœ€æœ‰ç”¨çš„åŠŸèƒ½è¿›è¡Œåˆæ­¥å¼€å‘ï¼Œå¹¶æ›´æœ‰ä¿¡å¿ƒæ‚¨çš„æ—¶é—´å°†å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨çš„æŒ‡æ ‡ç§°ä¸ºâ€œç›¸äº’ä¿¡æ¯â€ã€‚äº’ä¿¡æ¯å¾ˆåƒç›¸å…³æ€§ï¼Œå› ä¸ºå®ƒè¡¡é‡ä¸¤ä¸ªé‡ä¹‹é—´çš„å…³ç³»ã€‚
äº’ä¿¡æ¯çš„ä¼˜ç‚¹æ˜¯å®ƒå¯ä»¥æ£€æµ‹ä»»ä½•ç±»å‹çš„å…³ç³»ï¼Œè€Œç›¸å…³æ€§åªèƒ½æ£€æµ‹çº¿æ€§å…³ç³»ã€‚

äº’ä¿¡æ¯æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€šç”¨æŒ‡æ ‡ï¼Œåœ¨åŠŸèƒ½å¼€å‘å¼€å§‹æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºä½ å¯èƒ½è¿˜ä¸çŸ¥é“ä½ æƒ³ä½¿ç”¨ä»€ä¹ˆæ¨¡å‹ã€‚

ä¼˜ç‚¹ï¼š
æ˜“äºä½¿ç”¨å’Œè§£é‡Šï¼Œè®¡ç®—æ•ˆç‡ï¼Œç†è®ºä¸Šæœ‰å……åˆ†çš„æ ¹æ®ï¼ŒæŠ—è¿‡æ‹Ÿåˆï¼Œä»¥åŠèƒ½å¤Ÿæ£€æµ‹ä»»ä½•ç±»å‹çš„å…³ç³»
tips:æœ‰å…³è¿™ä¸ªæ ‡é¢˜çš„ä»£ç ï¼Œå‚¨å­˜åœ¨ml.ipynb

# Create Features
ç ”ç©¶æ•°æ®é›†é¢†åŸŸç›¸å…³çš„çŸ¥è¯†ï¼Œæ¥å¯»æ±‚æ–°çš„æ´¾ç”Ÿç‰¹å¾

Mathematical Transforms

æ•°å€¼ç‰¹å¾ä¹‹é—´çš„å…³ç³»é€šå¸¸é€šè¿‡æ•°å­¦å…¬å¼æ¥è¡¨ç¤ºï¼Œä½œä¸ºé¢†åŸŸç ”ç©¶çš„ä¸€éƒ¨åˆ†ï¼Œæ‚¨ç»å¸¸ä¼šé‡åˆ°è¿™äº›å…¬å¼ã€‚
åœ¨ Pandas ä¸­ï¼Œæ‚¨å¯ä»¥å°†ç®—æœ¯è¿ç®—åº”ç”¨äºåˆ—ï¼Œå°±åƒå®ƒä»¬æ˜¯æ™®é€šæ•°å­—ä¸€æ ·ã€‚




