{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding\n",
    "# 我们在本课程中看到的大多数技术都是针对数值特征的。我们将在本课中介绍的技术（目标编码）是针对分类特征的。\n",
    "# 它是一种将类别编码为数字的方法，如单热编码或标签编码，不同之处在于它还使用目标来创建编码。\n",
    "# 这使它成为我们所说的监督特征工程技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "autos = pd.read_csv(\"../input/fe-course-data/autos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标编码¶\n",
    "# 目标编码是将要素的类别替换为从目标派生的某个数字的任何类型的编码。\n",
    "\n",
    "# 一个简单而有效的版本是应用第 3 课中的组聚合，例如平均值。使用汽车数据集，可以计算每辆车的平均价格：\n",
    "autos[\"make_encoded\"] = autos.groupby(\"make\")[\"price\"].transform(\"mean\")\n",
    "\n",
    "autos[[\"make\", \"price\", \"make_encoded\"]].head(10)\n",
    "\n",
    "# 这种目标编码有时称为均值编码。应用于二进制目标，也称为箱数计数。（您可能会遇到的其他名称包括：似然编码、影响编码和遗漏一例外编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平滑¶\n",
    "# 组转换形成的编码会带来一些问题。首先是未知类别。\n",
    "# 目标编码会产生过拟合的特殊风险，这意味着它们需要在独立的“编码”拆分上进行训练。\n",
    "# 当您将编码加入到将来的拆分中时，Pandas 将填充编码拆分中不存在的任何类别的缺失值。这些缺失的值，你必须以某种方式插补。\n",
    "\n",
    "# 其次是稀有类别。当一个类别在数据集中只出现几次时，根据其组计算的任何统计数据都不太可能非常准确。\n",
    "# 在 Automobiles 数据集中，mercurcy make 仅发生一次。\n",
    "# 我们计算的“平均”价格只是这辆车的价格，它可能不太能代表我们将来可能看到的任何 Mercuries。对稀有类别进行目标编码会使过度拟合的可能性更大。\n",
    "\n",
    "# 解决这些问题的方法是添加平滑。这个想法是将类别内平均值与总体平均值混合在一起。\n",
    "# 稀有类别在其类别平均值上的权重较小，而缺失类别仅获得总体平均值。\n",
    "\n",
    "# 在伪代码中：\n",
    "'encoding = weight * in_category + (1 - weight) * overall'\n",
    "# 'in_category': 类别内平均值(在这个示例中)\n",
    "# 其中，权重是根据类别频率计算得出的介于 0 和 1 之间的值。\n",
    "\n",
    "# 确定权重值的一种简单方法是计算 m 估计值：\n",
    "'weight = n / (n + m)'\n",
    "\n",
    "# 其中 n 是该类别在数据中出现的总次数。参数 m 确定“平滑因子”。m 值越大，总体估计值的权重越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标编码的用例\n",
    "# 目标编码非常适合：\n",
    "# 高基数特征：具有大量类别的特征可能很难编码：单热编码会生成太多特征，而替代方法（如标签编码）可能不适合该特征。\n",
    "# 目标编码使用要素最重要的属性（其与目标的关系）派生类别的编号。\n",
    "# 领域激励功能：根据以前的经验，您可能会怀疑分类功能应该很重要，即使它在功能指标中得分很低。\n",
    "# 目标编码可以帮助揭示特征的真实信息性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieLens1M\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/movielens1m.csv\")\n",
    "df = df.astype(np.uint8, errors='ignore') # reduce memory footprint\n",
    "print(\"Number of Unique Zipcodes: {}\".format(df[\"Zipcode\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With over 3000 categories, the Zipcode feature makes a good candidate for target encoding, \n",
    "# and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\n",
    "\n",
    "# We'll start by creating a 25% split to train the target encoder.\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('Rating')\n",
    "\n",
    "X_encode = X.sample(frac=0.25)\n",
    "y_encode = y[X_encode.index]\n",
    "X_pretrain = X.drop(X_encode.index)\n",
    "y_train = y[X_pretrain.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The category_encoders package in scikit-learn-contrib implements an m-estimate encoder, \n",
    "# which we'll use to encode our Zipcode feature.\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Create the encoder instance. Choose m to control noise.\n",
    "encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# Fit the encoder on the encoding split.\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the Zipcode column to create the final training data\n",
    "X_train = encoder.transform(X_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the encoded values to the target to see how informative our encoding might be.\n",
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y, kde=False, norm_hist=True)\n",
    "ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.legend(labels=['Zipcode', 'Rating']);\n",
    "\n",
    "# 编码的邮政编码功能的分布大致遵循实际评级的分布，\n",
    "# 这意味着电影观众的评级在邮政编码之间的差异很大，以至于我们的目标编码能够捕获有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../input/fe-course-data/ames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you'll need to choose which features you want to apply a target encoding to. \n",
    "# Categorical features with a large number of categories are often good candidates. \n",
    "# Run this cell to see how many categories each categorical feature in the Ames dataset has.\n",
    "df.select_dtypes([\"object\"]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We talked about how the M-estimate encoding uses smoothing to improve estimates for rare categories. \n",
    "# To see how many times a category occurs in the dataset, you can use the value_counts method. \n",
    "# This cell shows the counts for SaleType, but you might want to consider others as well.\n",
    "df[\"SaleType\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Features for Encoding\n",
    "# Which features did you identify for target encoding? \n",
    "\n",
    "# 每次训练都要单独创造训练集来训练\n",
    "# Now you'll apply a target encoding to your choice of feature. \n",
    "# As we discussed in the tutorial, to avoid overfitting, we need to fit the encoder on data heldout from the training set. \n",
    "# Run this cell to create the encoding and training splits:\n",
    "\n",
    "\n",
    "# Encoding split\n",
    "X_encode = df.sample(frac=0.20, random_state=0)\n",
    "y_encode = X_encode.pop(\"SalePrice\")\n",
    "\n",
    "# Training split\n",
    "X_pretrain = df.drop(X_encode.index)\n",
    "y_train = X_pretrain.pop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply M-Estimate Encoding\n",
    "# Apply a target encoding to your choice of categorical features. \n",
    "# Also choose a value for the smoothing parameter m (any value is okay for a correct answer).\n",
    "# Choose a set of features to encode and a value for m\n",
    "encoder = MEstimateEncoder(cols=[\"Neighborhood\"], m=3.0)\n",
    "\n",
    "\n",
    "# Fit the encoder on the encoding split\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the training split\n",
    "X_train = encoder.transform(X_pretrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature = encoder.cols\n",
    "\n",
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y_train, kde=True, hist=False)\n",
    "ax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\n",
    "ax.set_xlabel(\"SalePrice\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "score_base = score_dataset(X, y)\n",
    "score_new = score_dataset(X_train, y_train)\n",
    "\n",
    "print(f\"Baseline Score: {score_base:.4f} RMSLE\")\n",
    "print(f\"Score with Encoding: {score_new:.4f} RMSLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you think that target encoding was worthwhile in this case? \n",
    "# Depending on which feature or features you chose, you may have ended up with a score significantly worse than the baseline. \n",
    "# In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding.\n",
    "\n",
    "# In this question, you'll explore the problem of overfitting with target encodings. This will illustrate this importance of training fitting target encoders on data held-out from the training set.\n",
    "\n",
    "# So let's see what happens when we fit the encoder and the model on the same dataset. \n",
    "# To emphasize how dramatic the overfitting can be, we'll mean-encode a feature that should have no relationship with SalePrice, a count: 0, 1, 2, 3, 4, 5, ....\n",
    "\n",
    "# Try experimenting with the smoothing parameter m\n",
    "# Try 0, 1, 5, 50\n",
    "m = 50\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('SalePrice')\n",
    "\n",
    "# Create an uninformative feature\n",
    "X[\"Count\"] = range(len(X))\n",
    "X[\"Count\"][1] = 0  # actually need one duplicate value to circumvent error-checking in MEstimateEncoder\n",
    "\n",
    "# fit and transform on the same dataset\n",
    "encoder = MEstimateEncoder(cols=\"Count\", m=m)\n",
    "X = encoder.fit_transform(X, y)\n",
    "\n",
    "# Results\n",
    "score =  score_dataset(X, y)\n",
    "print(f\"Score: {score:.4f} RMSLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y, kde=True, hist=False)\n",
    "ax = sns.distplot(X[\"Count\"], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\n",
    "ax.set_xlabel(\"SalePrice\");\n",
    "\n",
    "# Since Count never has any duplicate values, the mean-encoded Count is essentially an exact copy of the target. In other words, mean-encoding turned a completely meaningless feature into a perfect feature.\n",
    "\n",
    "# Now, the only reason this worked is because we trained XGBoost on the same set we used to train the encoder. If we had used a hold-out set instead, none of this \"fake\" encoding would have transferred to the training data.\n",
    "\n",
    "# The lesson is that when using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
