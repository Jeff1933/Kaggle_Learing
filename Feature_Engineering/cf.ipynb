{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "accidents = pd.read_csv(\"../input/fe-course-data/accidents.csv\")\n",
    "autos = pd.read_csv(\"../input/fe-course-data/autos.csv\")\n",
    "concrete = pd.read_csv(\"../input/fe-course-data/concrete.csv\")\n",
    "customer = pd.read_csv(\"../input/fe-course-data/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Transforms 数学转换\n",
    "# 汽车数据集\n",
    "# “冲程比”是衡量发动机效率与性能的指标：\n",
    "autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n",
    "\n",
    "autos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()\n",
    "\n",
    "# 组合越复杂，模型就越难学习，就像发动机“排量”的公式一样，这是衡量其功率的指标：\n",
    "autos[\"displacement\"] = (\n",
    "    np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据可视化可以建议转换，通常是通过幂或对数对特征进行“重塑”。例如，WindSpeed在美国事故中的分布高度倾斜。\n",
    "# 在这种情况下，对数可以有效地对其进行归一化：数据可视化可以建议变换，通常是通过幂或对数对特征进行“重塑”。\n",
    "# 例如，WindSpeed在美国事故中的分布高度倾斜。在这种情况下，对数可以有效地对其进行归一化：\n",
    "\n",
    "\n",
    "# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# Plot a comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1])\n",
    "\n",
    "# 另外我们可以使用Box-Cox归一器，这点在datacleaning中有提到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计数¶\n",
    "# 描述某物存在与否的特征通常成组出现，例如疾病的风险因素集。您可以通过创建计数来聚合此类要素。\n",
    "\n",
    "# 这些特征将是二进制（1 表示存在，0 表示不存在）或布尔值（True 或 False）。在 Python 中，布尔值可以像整数一样相加。\n",
    "\n",
    "# 在交通事故中，有几个特征表明某些道路物体是否在事故附近。这将使用 sum 方法创建附近道路要素总数的计数：\n",
    "roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"TrafficCalming\", \"TrafficSignal\"]\n",
    "accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n",
    "\n",
    "accidents[roadway_features + [\"RoadwayFeatures\"]].head(10)\n",
    "\n",
    "# 尽管对不是bool类型的特征我们也可以对其进行计数，\n",
    "# 使用 DataFrame 的内置方法来创建布尔值。在混凝土数据集中是混凝土配方中的组分量。\n",
    "# 许多配方缺少一个或多个组分（即，该组分的值为 0）。\n",
    "# 这将使用 DataFrame 的内置 greater-than gt 方法计算公式中有多少个组件：\n",
    "components = [ \"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\",\n",
    "               \"Superplasticizer\", \"CoarseAggregate\", \"FineAggregate\"]\n",
    "concrete[\"Components\"] = concrete[components].gt(0).sum(axis=1)\n",
    "\n",
    "concrete[components + [\"Components\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building-Up and Breaking-Down Features¶\n",
    "# Often you'll have complex strings that can usefully be broken into simpler pieces. Some common examples:\n",
    "# ID numbers: '123-45-6789'\n",
    "# Phone numbers: '(999) 555-0123'\n",
    "# Street addresses: '8241 Kaggle Ln., Goose City, NV'\n",
    "# Internet addresses: 'http://www.kaggle.com\n",
    "# Product codes: '0 36000 29145 2'\n",
    "# Dates and times: 'Mon Sep 30 07:06:05 2013'\n",
    "\n",
    "# The str accessor lets you apply string methods like split directly to columns. \n",
    "# The Customer Lifetime Value dataset contains features describing customers of an insurance company. \n",
    "# From the Policy feature, we could separate the Type from the Level of coverage:\n",
    "customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "    customer[\"Policy\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\" \", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")\n",
    "\n",
    "customer[[\"Policy\", \"Type\", \"Level\"]].head(10)\n",
    "\n",
    "# combine features\n",
    "autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\n",
    "autos[[\"make\", \"body_style\", \"make_and_style\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组转换¶\n",
    "# 最后，我们有组转换，它聚合了按某个类别分组的多行的信息。\n",
    "# 通过组转换，您可以创建诸如“一个人居住州的平均收入”或“按类型划分的工作日上映的电影比例”等功能。\n",
    "# 如果您发现了类别交互，则对该类别进行组转换可能是值得调查的好事。\n",
    "\n",
    "# 使用聚合函数，组转换将两个特征组合在一起：一个提供分组的分类特征，另一个特征要聚合其值。\n",
    "# 对于“按州划分的平均收入”，您可以为分组要素选择州，为聚合函数选择均值，为聚合要素选择收入。\n",
    "# 为了在 Pandas 中计算这一点，我们使用 groupby 和 transform 方法：\n",
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)\n",
    "\n",
    "# The mean function is a built-in dataframe method, which means we can pass it as a string to transform. \n",
    "# Other handy methods include max, min, median, var, std, and count. \n",
    "# Here's how you could calculate the frequency with which each state occurs in the dataset:每个州出现的频率\n",
    "customer[\"StateFreq\"] = (\n",
    "    customer.groupby(\"State\")\n",
    "    [\"State\"]\n",
    "    .transform(\"count\")\n",
    "    / customer.State.count()\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"StateFreq\"]].head(10)\n",
    "# You could use a transform like this to create a \"frequency encoding\" for a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using training and validation splits, to preserve their independence, \n",
    "# it's best to create a grouped feature using only the training set and then join it to the validation set. \n",
    "# We can use the validation set's merge method after creating a unique set of values with drop_duplicates on the training set:\n",
    "# Create splits\n",
    "df_train = customer.sample(frac=0.5)\n",
    "df_valid = customer.drop(df_train.index)\n",
    "\n",
    "# Create the average claim amount by coverage type, on the training set\n",
    "df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n",
    "\n",
    "# Merge the values into the validation set\n",
    "df_valid = df_valid.merge(\n",
    "    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n",
    "    on=\"Coverage\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_valid[[\"Coverage\", \"AverageClaim\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tips on Creating Features\n",
    "It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(\"../input/fe-course-data/ames.csv\")\n",
    "X = df.copy()\n",
    "y = X.pop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们从几个数学组合开始。我们将重点介绍描述区域的特征 - 具有相同的单位（平方英尺）可以很容易地以合理的方式组合它们。\n",
    "# 由于我们使用的是 XGBoost（基于树的模型），因此我们将重点关注比率和总和。\n",
    "X_1 = pd.DataFrame()  # dataframe to hold new features\n",
    "\n",
    "X_1[\"LivLotRatio\"] = X[\"GrLivArea\"] / X.LotArea\n",
    "X_1[\"Spaciousness\"] = (X.FirstFlrSF + X.SecondFlrSF) / X.TotRmsAbvGrd\n",
    "X_1[\"TotalOutsideSF\"] = X.WoodDeckSF + X.OpenPorchSF + X.EnclosedPorch + X.Threeseasonporch + X.ScreenPorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果发现数值特征和分类特征之间存在交互效应，则可能需要使用单热编码对其进行显式建模，如下所示：\n",
    "# Categorical是分类特征，Continuous是数值特征\n",
    "# One-hot encode Categorical feature, adding a column prefix \"Cat\"\n",
    "X_new = pd.get_dummies(df.Categorical, prefix=\"Cat\")\n",
    "\n",
    "# Multiply row-by-row\n",
    "X_new = X_new.mul(df.Continuous, axis=0)\n",
    "\n",
    "# Join the new features to the feature set\n",
    "X = X.join(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode BldgType. Use `prefix=\"Bldg\"` in `get_dummies`\n",
    "X_2 = pd.get_dummies(X.BldgType, prefix=\"Bldg\") \n",
    "# Multiply\n",
    "X_2 = X_2.mul(X.GrLivArea, axis=0)\n",
    "print(X_2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Feature\n",
    "X_3 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "components = [\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"Threeseasonporch\",\"ScreenPorch\"]\n",
    "X_3[\"PorchTypes\"] = X[components].gt(0).sum(axis=1)\n",
    "print(X_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break Down a Categorical Feature\n",
    "df.MSSubClass.unique()\n",
    "\n",
    "X_4 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_4[\"MSClass\"] = X[\"MSSubClass\"].str.split(\"_\", n=1, expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Grouped Transform\n",
    "# 房屋的价值通常取决于它与附近典型房屋的比较情况。创建一个要素 MedNhbdArea，用于描述在邻域上分组的 GrLivArea 的中位数。\n",
    "X_5 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_5[\"MedNhbdArea\"] = (\n",
    "                    X.groupby(\"Neighborhood\")\n",
    "                    [\"GrLivArea\"].transform(\"median\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(X, y)\n",
    "X_new = X.join([X_1, X_2, X_3, X_4, X_5])\n",
    "score_dataset(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    score = cross_val_score(\n",
    "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
